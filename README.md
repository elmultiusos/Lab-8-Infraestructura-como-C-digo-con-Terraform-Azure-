# Lab #8 â€” Infraestructura como CÃ³digo con Terraform (Azure)

**Curso:** BluePrints / ARSW  
**Estudiante:** Juan Buitrago (@elmultiusos)  
**Ãšltima actualizaciÃ³n:** 2025-11-15  
**Estado:** âœ… Infraestructura desplegada y funcionando

> ğŸ¯ **Load Balancer pÃºblico:** http://4.157.249.175

---

## ğŸ“Š Resumen de ImplementaciÃ³n

### Infraestructura Desplegada

- **Resource Group:** `lab8-rg` (East US)
- **Virtual Network:** `lab8-vnet` (10.10.0.0/16)
  - Subnet Web: 10.10.1.0/24
  - Subnet Mgmt: 10.10.2.0/24
- **Load Balancer:** `lab8-lb` (Standard, IP: 4.157.249.175)
- **Virtual Machines:** 2x Ubuntu 22.04 LTS (Standard_B1s) con nginx
- **Backend remoto:** Azure Storage (`sttfstateelmultiusos`)

---

## PropÃ³sito

Modernizar el laboratorio de balanceo de carga en Azure usando **Terraform** para definir, aprovisionar y versionar la infraestructura. El objetivo es que los estudiantes diseÃ±en y desplieguen una arquitectura reproducible, segura y con buenas prÃ¡cticas de _IaC_.

## Objetivos de aprendizaje

1. Modelar infraestructura de Azure con Terraform (providers, state, mÃ³dulos y variables).
2. Desplegar una arquitectura de **alta disponibilidad** con **Load Balancer** (L4) y 2+ VMs Linux.
3. Endurecer mÃ­nimamente la seguridad: **NSG**, **SSH por clave**, **tags**, _naming conventions_.
4. Integrar **backend remoto** para el _state_ en Azure Storage con _state locking_.
5. Automatizar _plan_/**apply** desde **GitHub Actions** con autenticaciÃ³n OIDC (sin secretos largos).
6. Validar operaciÃ³n (health probe, pÃ¡gina de prueba), observar costos y destruir con seguridad.

> **Nota:** Este lab reemplaza la versiÃ³n clÃ¡sica basada en acciones manuales. EnfÃ³cate en _IaC_ y _pipelines_.

---

## Arquitectura objetivo

- **Resource Group** (p. ej. `rg-lab8-<alias>`)
- **Virtual Network** con 2 subredes:
  - `subnet-web`: VMs detrÃ¡s de **Azure Load Balancer (pÃºblico)**
  - `subnet-mgmt`: Bastion o salto (opcional)
- **Network Security Group**: solo permite **80/TCP** (HTTP) desde Internet al LB y **22/TCP** (SSH) solo desde tu IP pÃºblica.
- **Load Balancer** pÃºblico:
  - Frontend IP pÃºblica
  - Backend pool con 2+ VMs
  - **Health probe** (TCP/80 o HTTP)
  - **Load balancing rule** (80 â†’ 80)
- **2+ VMs Linux** (Ubuntu LTS) con cloud-init/Custom Script Extension para instalar **nginx** y servir una pÃ¡gina con el **hostname**.
- **Azure Storage Account + Container** para Terraform **remote state** (con bloqueo).
- **Etiquetas (tags)**: `owner`, `course`, `env`, `expires`.

> **Opcional** (retos): usar **VM Scale Set**, o reemplazar LB por **Application Gateway** (L7).

---

## Requisitos previos

- Cuenta/Subscription en Azure (Azure for Students o equivalente).
- **Azure CLI** (`az`) y **Terraform >= 1.6** instalados en tu equipo.
- **SSH key** generada (ej. `ssh-keygen -t ed25519`).
- Cuenta en **GitHub** para ejecutar el pipeline de Actions.

---

## Estructura del repositorio (sugerida)

```
.
â”œâ”€ infra/
â”‚  â”œâ”€ main.tf
â”‚  â”œâ”€ providers.tf
â”‚  â”œâ”€ variables.tf
â”‚  â”œâ”€ outputs.tf
â”‚  â”œâ”€ backend.hcl.example
â”‚  â”œâ”€ cloud-init.yaml
â”‚  â””â”€ env/
â”‚     â”œâ”€ dev.tfvars
â”‚     â””â”€ prod.tfvars (opcional)
â”œâ”€ modules/
â”‚  â”œâ”€ vnet/
â”‚  â”‚  â”œâ”€ main.tf
â”‚  â”‚  â”œâ”€ variables.tf
â”‚  â”‚  â””â”€ outputs.tf
â”‚  â”œâ”€ compute/
â”‚  â”‚  â”œâ”€ main.tf
â”‚  â”‚  â”œâ”€ variables.tf
â”‚  â”‚  â””â”€ outputs.tf
â”‚  â””â”€ lb/
â”‚     â”œâ”€ main.tf
â”‚     â”œâ”€ variables.tf
â”‚     â””â”€ outputs.tf
â””â”€ .github/workflows/terraform.yml
```

---

## Bootstrap del backend remoto

Primero crea el **Resource Group**, **Storage Account** y **Container** para el _state_:

```bash
# Nombres Ãºnicos
SUFFIX=$RANDOM
LOCATION=eastus
RG=rg-tfstate-lab8
STO=sttfstate${SUFFIX}
CONTAINER=tfstate

az group create -n $RG -l $LOCATION
az storage account create -g $RG -n $STO -l $LOCATION --sku Standard_LRS --encryption-services blob
az storage container create --name $CONTAINER --account-name $STO
```

Completa `infra/backend.hcl.example` con los valores creados y renÃ³mbralo a `backend.hcl`.

---

## Variables principales (ejemplo)

En `infra/variables.tf` define:

- `prefix`, `location`, `vm_count`, `admin_username`, `ssh_public_key`
- `allow_ssh_from_cidr` (tu IPv4 en /32)
- `tags` (map)

En `infra/env/dev.tfvars`:

```hcl
prefix        = "lab8"
location      = "eastus"
vm_count      = 2
admin_username= "student"
ssh_public_key= "~/.ssh/id_ed25519.pub"
allow_ssh_from_cidr = "X.X.X.X/32" # TU IP
tags = { owner = "tu-alias", course = "ARSW/BluePrints", env = "dev", expires = "2025-12-31" }
```

---

## cloud-init de las VMs

Archivo `infra/cloud-init.yaml` (instala nginx y muestra el hostname):

```yaml
#cloud-config
package_update: true
packages:
  - nginx
runcmd:
  - echo "Hola desde $(hostname)" > /var/www/html/index.nginx-debian.html
  - systemctl enable nginx
  - systemctl restart nginx
```

---

## Flujo de trabajo local

```bash
cd infra

# AutenticaciÃ³n en Azure
az login
az account show # verifica la suscripciÃ³n activa

# Inicializa Terraform con backend remoto
terraform init -backend-config=backend.hcl

# RevisiÃ³n rÃ¡pida
terraform fmt -recursive
terraform validate

# Plan con variables de dev
terraform plan -var-file=env/dev.tfvars -out plan.tfplan

# Apply
terraform apply "plan.tfplan"

# Verifica el LB pÃºblico (cambia por tu IP)
curl http://$(terraform output -raw lb_public_ip)
```

**Outputs esperados** (ejemplo):

- `lb_public_ip`
- `resource_group_name`
- `vm_names`

---

## GitHub Actions (CI/CD con OIDC)

El _workflow_ `.github/workflows/terraform.yml`:

- Ejecuta `fmt`, `validate` y `plan` en cada PR.
- Publica el plan como artefacto/comentario.
- Job manual `apply` con _workflow_dispatch_ y aprobaciÃ³n.

**Configura OIDC** en Azure (federaciÃ³n con tu repositorio) y asigna el rol **Contributor** al _principal_ del _workflow_ sobre el RG del lab.

---

## Entregables en TEAMS

1. **Repositorio GitHub** del equipo con:
   - CÃ³digo Terraform (mÃ³dulos) y `cloud-init.yaml`.
   - `backend.hcl` **(sin secretos)** y `env/dev.tfvars` (sin llaves privadas).
   - Workflow de GitHub Actions y evidencias del `plan`.
2. **Diagrama** (componente y secuencia) del caso de estudio propuesto.
3. **URL/IP pÃºblica** del Load Balancer + **captura** mostrando respuesta de **2 VMs** (p. ej. refrescar y ver hostnames cambiar).
4. **ReflexiÃ³n tÃ©cnica** (1 pÃ¡gina mÃ¡x.): decisiones, tradeâ€‘offs, costos aproximados y cÃ³mo destruir seguro.
5. **Limpieza**: confirmar `terraform destroy` al finalizar.

---

## RÃºbrica (100 pts)

- **Infra desplegada y funcional (40 pts):** LB, 2+ VMs, health probe, NSG correcto.
- **Buenas prÃ¡cticas Terraform (20 pts):** mÃ³dulos, variables, `fmt/validate`, _remote state_.
- **Seguridad y costos (15 pts):** SSH por clave, NSG mÃ­nimo, tags y _naming_; estimaciÃ³n de costos.
- **CI/CD (15 pts):** pipeline con `plan` automÃ¡tico y `apply` manual (OIDC).
- **DocumentaciÃ³n y diagramas (10 pts):** README del equipo, diagramas claros y reflexiÃ³n.

---

## Retos (elige 2+)

- Migrar a **VM Scale Set** con _Custom Script Extension_ o **cloud-init**.
- Reemplazar LB por **Application Gateway** con _probe_ HTTP y _path-based routing_ (si exponen mÃºltiples apps).
- **Azure Bastion** para acceso SSH sin IP pÃºblica en VMs.
- **Alertas** de Azure Monitor (p. ej. estado del probe) y **Budget alert**.
- **MÃ³dulos privados** versionados con _semantic versioning_.

---

## Limpieza

```bash
terraform destroy -var-file=env/dev.tfvars
```

> **Tip:** MantÃ©n los recursos etiquetados con `expires` y **elimina** todo al terminar.

---

## Preguntas de reflexiÃ³n

- Â¿Por quÃ© L4 LB vs Application Gateway (L7) en tu caso? Â¿QuÃ© cambiarÃ­a?
- Â¿QuÃ© implicaciones de seguridad tiene exponer 22/TCP? Â¿CÃ³mo mitigarlas?
- Â¿QuÃ© mejoras harÃ­as si esto fuera **producciÃ³n**? (resiliencia, autoscaling, observabilidad).

---

## CrÃ©ditos y material de referencia

- Azure, Terraform, IaC, LB y VMSS (docs oficiales) â€” revisa enlaces en clase.
